{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "     # Practical Machine Learning with Python on Watson Studio\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Introduction to Machine Learning\n",
    "    - What is Machine Learning?\n",
    "    - How to do Machine Learning?\n",
    "\n",
    "2. Hands-on Workshop\n",
    "    - Predicting Employee Attrition use case\n",
    "        - Notebooks\n",
    "        - Modeler flow\n",
    "        - Auto AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Workshop materials: https://ibm.box.com/s/9qjyr0jrk2y4smoi72ry3yhcxge1azi8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Machine Learning?\n",
    "![ML](https://ibm.box.com/shared/static/7ah4wdg9cur8orm6dcwrsep816pdvigy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the difference between conventional programming and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Brain exercise: If you were given a task to write a program to determinate if a given string is a valid email address or not, how would you approach the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Rule](https://ibm.box.com/shared/static/6cb4qfoegt4sxo5j0swfc5l58x5apoke.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![ML](https://ibm.box.com/shared/static/4526ns06fyjiysz33o07syuj20d1zbs5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![DL](https://ibm.box.com/shared/static/rz94quty92fl7ingnkavpvkv0br2mr5n.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Different Types of Machine Learning\n",
    "![AI](https://miro.medium.com/max/899/1*9Eu_-DDMZ_bP_t94_MMEYA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Why is Machine Learning achieving a lot of success over the last few years?\n",
    "\n",
    "- Availability of Big Data\n",
    "- Modern algorithms + more data = better performance\n",
    "- Supercomputing power to process this Big Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to do Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML Workflow\n",
    "![Workflow](https://ibm.box.com/shared/static/vrocgtj81vzbym3plxrkynmntlg7jmgv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Elements of ML Success\n",
    "\n",
    "![Success](https://ibm.box.com/shared/static/pt7n246a1zaepap9lfi01b22ef4xkncs.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>What We Will Practice Today?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='top'></a>\n",
    "<h2>Table of Contents</h2>\n",
    "\n",
    "- <a href=#pythonbasics>Python Basics</a>\n",
    "- <a href=#numpanda>Numpy and Pandas</a>\n",
    "- <a href=#libs> 1. Libraries and Helper Functions </a>\n",
    "- <a href=#getdata> 2. Loading Dataset</a>\n",
    "- <a href=#dataexplore>3. Exploratory Data Analysis</a>\n",
    "- <a href=#Featureeng>4. Feature Engineering</a>\n",
    "- <a href=#Featureselec>5. Feature Selection</a>\n",
    "- <a href=#modeling>6. Modeling</a>\n",
    "- <a href=#modeleval>7. Model Evaluation</a>\n",
    "- <a href=#inference>8. Inference and Conclusions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Jupyter Shortcuts <br>\n",
    "- **shift + enter** - run cell, select below.<br>\n",
    "- **ctrl + enter** - run cell.<br>\n",
    "- **option + enter** - run cell, insert below.<br>\n",
    "- **a** - insert cell above.<br>\n",
    "- **b** - insert cell below.<br>\n",
    "- **c** - copy cell.<br>\n",
    "- **v** - paste cell.<br>\n",
    "- **dd** - delete selected cell.<br>\n",
    "- **m** - markdown\n",
    "- **Enter** - go into Edit Mode\n",
    "- **ii** - interrupt kernel\n",
    "- **oo** - restart kernel\n",
    "- **shift+tab** within braces - help on parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1> Why Python? </h1>\n",
    "<img src=\"https://ibm.box.com/shared/static/bm12badb0gfzo6b8gob46haxtysazhkf.png\" style=\"width:800px;height:400px;\" align=\"right\"/>\n",
    "- **Multipurpose**: General purpose language\n",
    "- **Simplicity**: Elegant and readable code\n",
    "- **Support**: Large open source community\n",
    "- **Libraries**: Popular for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pythonbasics\"></a>\n",
    "# Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Your first Hello World! program\n",
    "\n",
    "In Python the indentation is very important and it is used to indicate a block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello to the World of Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comments\n",
    "Hasgtag is used for single line comment and triple quotes are used for multiple line comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# single line comment\n",
    "\n",
    "\"\"\" Comment \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variables\n",
    "To declare a variable in Python you just need to assign a value to it.\n",
    "Variables do not need to be declared with any particular type and can even change type after they have been set. Rules for variables:\n",
    "- A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )\n",
    "- A variable name cannot start with a number\n",
    "- Variable names are case-sensitive (age, Age and AGE are three different variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "var ='hello world of Machine Learning'\n",
    "VAR = \"hi\"\n",
    "print(var)\n",
    "print(VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(type(12))\n",
    "print(type(2.14))\n",
    "print(type(\"Hello Python 101\"))\n",
    "print(type(True))\n",
    "print(type(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sometimes you may want to specify a type on to a variable. This can be done with casting using constructor functions like **int()**, **str()**, **float()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(type(int(1.1)))\n",
    "print(type(str(1)))\n",
    "\n",
    "# from packagename import class_name\n",
    "from decimal import Decimal    \n",
    "Decimal(2.675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Arithmetic operators\n",
    "- **+** is addition: x+y\n",
    "- **-** is subtraction: x-y\n",
    "- **/** is division: x/y\n",
    "- **//** is floor division: x//y\n",
    "- ***** is multiplication: x*y\n",
    "- **%** is modulus: x%y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "total_hr = (43 + 42 + 57) / 60  # get total hours in a single expression\n",
    "total_hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison operators\n",
    "Use to compare two values. It returns boolean i.e **True** or **False**.\n",
    "- **==** is equal: x==y\n",
    "- **!=** is not equal: x!=y\n",
    "- **>** is greater than: x>y\n",
    "- **<** is less than: x<y\n",
    "- **>=** is greater than or equal to: x>=y\n",
    "- **<=** is less than or equal to: x<=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(3 > 5)\n",
    "print(3 == 3)\n",
    "print(3 == \"3\") # strings and integers are not equal, different data types\n",
    "print(\"ABC\"<\"abc\") # comparison operators are case sensitive lower-case > upper-case\n",
    "print(\"A\" < \"z\") # smallest char is capital A, largest char is lower-case z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strings\n",
    "Strings are surrounded either by double or by single quotation marks. \n",
    "The first character has the position 0. Use square brackets to access <i>elements of the string</i> i.e. <i>substrings</i>.\n",
    "<br>\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-a380b1bc159589df5e0b9842e5b56b6d\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Name= \"Michael Jackson\"\n",
    "print(Name[13])# character at 13th position\n",
    "print(Name[-1])# last character\n",
    "print(Name[0:4])# from 0 to 4 for index, last value excluded 0-3.\n",
    "print(Name[-4:]) # last 4 character of str\n",
    "print(Name[::-1])# reverse string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Strings have different methods:\n",
    "- The **upper()** method returns the string in upper case.\n",
    "- The **lower()** method returns the string in lower case.\n",
    "- The **len()** method returns the length of a string.\n",
    "- The **strip()** method removes any whitespace from the beginning or the end.\n",
    "- The **replace()** method replaces a string with another string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A=\"Thriller is the sixth studio album\"\n",
    "print(\"Before upper:\",A)\n",
    "B=A.upper()\n",
    "print(\"After upper:\",B)\n",
    "\n",
    "A=\"Michael Jackson is the best\"\n",
    "B=A.replace('Michael', 'Janet')\n",
    "print(\"String A is:\", A)\n",
    "print(\"String B is:\", B)\n",
    "\n",
    "print(len(A))\n",
    "print(len(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Functions\n",
    "- In Python a function is defined using the **def** keyword followed by the function name parameters inside the parentheses.\n",
    "- A function may or may not have parameters.\n",
    "- To call a function, use the function name followed by parenthesis.\n",
    "- To let a function return a value, use the **return** statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def type_of_album(artist,album,year_released):\n",
    "    if year_released > 1980:\n",
    "        #print(artist,album,year_released)\n",
    "        return \"Modern\"\n",
    "    else:\n",
    "        #print(artist,album,year_released)\n",
    "        return \"Oldie\"\n",
    "    \n",
    "x = type_of_album(\"Michael Jackson\",\"Thriller\",1980)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a name='numpanda' />\n",
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NumPy and Pandas<br>\n",
    "<strong>NumPy</strong> <br>\n",
    "- Open source library.\n",
    "- Working with high-performance arrays and matrices.<br>\n",
    "- Mathematical operations are vectorized.<br>\n",
    "- Main object is ndarray.<br>\n",
    "- Heavy computations (3+ dimension).<br>\n",
    "- Arrays are accessed by their integer position, starting with zero for the first element.<br>\n",
    "\n",
    "<strong>Pandas</strong> <br>\n",
    "- A package built around Numpy for data manipulation that uses the DataFrame objects.<br>\n",
    "- Series is the primary building block of panda.s<br>\n",
    "- Series Object is more flexible as you can use define your own labeled index and access elements of an array.<br>\n",
    "- Aligning data from different Series and matching labels (Joining) with Series objects is more efficient. For example dealing with missing values. If there are no matching labels during alignment, pandas returns NaN (not any number) so that the operation does not fail.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<strong>Note</strong>\n",
    "- Query-like operations work better in pandas as it has relational database concept.\n",
    "- Data manupulination is easier in pandas.\n",
    "- NumPy consumes less memory compared to pandas.\n",
    "- NumPy generally performs better than pandas for 50K rows or less.\n",
    "- Pandas generally performs better than numpy for 500K rows or more.\n",
    "- For 50K to 500K rows, it is a toss up between pandas and numpy depending on the kind of operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# To use libraries in Python first we must import them\n",
    "import numpy as np\n",
    "# Define a NumPy array\n",
    "a1 = np.array([1, 2, 3, 4, 5])\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorized mathematical operations in NumPy\n",
    "print(np.mean(a1))\n",
    "print(np.sum(a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import pandas as pd\n",
    "csv_path='https://ibm.box.com/shared/static/keo2qz0bvh4iu6gf5qjq4vdrkt67bvvb.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Display first 3 rows/observations; default is 5\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Display last 5 rows/observations\n",
    "df.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Shows data type and number of non-null observations per column\n",
    "df.info()\n",
    "# The size of the data (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are {} features/columns and {} rows/observations.\".format(df.shape[1],df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Number of unique values\n",
    "df['Released'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Unique values\n",
    "df['Released'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Unique values and their counts\n",
    "df['Released'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select column 'Artist'\n",
    "x = df['Artist']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns 'Artist','Length','Genre' and create new dataframe x\n",
    "x = df[['Artist','Length','Genre']]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Slice df\n",
    "print(df.iloc[0,0]) # 1st row and first column\n",
    "print(df.iloc[1,0]) # 2nd row and first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Slice df\n",
    "df.iloc[0:3, 0:3] # last index is exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set index to column with unique values\n",
    "df.set_index(\"Album\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering using loc\n",
    "a=df.loc['Thriller']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering using loc\n",
    "b=df.loc[['Thriller','Back in Black']]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Conditional filtering\n",
    "c=df.loc[df['Artist']=='Pink Floyd']\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Conditional filtering\n",
    "d=df.loc[df['Artist']=='Pink Floyd',['Genre']]\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Change observation value\n",
    "e=df.loc[df['Artist']=='Pink Floyd', \"Released\"] = \"1970\"\n",
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Summarize information using groupby and arithmetic functions\n",
    "df.groupby(['Released'])['Claimed Sales (millions)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of a dataframe\n",
    "df1=df.copy()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue\"><center> HR Analytics: Employee Churn Prediction </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee churn is a costly problem for companies. The true cost of replacing an employee\n",
    "can often be quite large. A study by the [Center for American Progress](https://www.americanprogress.org/wp-content/uploads/2012/11/CostofTurnover.pdf) found that companies typically pay about one-fifth of an employee’s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. <br>\n",
    "\n",
    "In this study, we will attempt to solve the following problem statements: <br>\n",
    "> What is the likelihood of an active employee leaving the company? <br>\n",
    "What are the key indicators of an employee leaving the company? <br>\n",
    "What policies or strategies can be adopted based on the results to improve employee retention?\n",
    "\n",
    "Given that we have data on former employees, this is a standard **Supervised Classification Problem** where the label **Attrition** is a binary variable: Yes (former employee), No (active employee).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"libs\"></a>\n",
    "# 1. Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "## Modeling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Performance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "\n",
    "## Other \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_discrete(df, feats, target, plot_title=\"\",grid_width = 3):\n",
    "    \"\"\"Analyzes relationship between target and discrete features.\"\"\"\n",
    "    sns.set_style('darkgrid')\n",
    "    rows = math.ceil(len(feats)/grid_width)\n",
    "    figwidth = 10 * grid_width\n",
    "    figheight = 5 * rows\n",
    "    fig, ax = plt.subplots(nrows = rows, ncols = grid_width, figsize = (figwidth, figheight))\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    ax = ax.ravel() # ravel turns a matrix into a vector, so that it is easier to iterate\n",
    "    \n",
    "    df = df.copy()\n",
    "    for i, feat in enumerate(feats):\n",
    "        df_to_plot = df.loc[df[\"Attrition\"] == \"Yes\", feat].value_counts()/df[feat].value_counts()*100\n",
    "        df_annot = np.round(df_to_plot).values\n",
    "        \n",
    "        ax[i].bar(x=df_to_plot.index, height=df_to_plot.values, color = np.random.rand(3,))\n",
    "        ax[i].set_ylabel(f'{target}', fontsize=14)\n",
    "        ax[i].set_xlabel(f'{feat}', fontsize=14)\n",
    "        rects = ax[i].patches\n",
    "        for rect, label in zip(rects, df_annot):\n",
    "            height = rect.get_height()\n",
    "            ax[i].text(rect.get_x() + rect.get_width(), height, label, ha='center', va='bottom')\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=10)\n",
    "            \n",
    "    fig.suptitle(f\"\\n{plot_title}\", size=25)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0, top=0.88)\n",
    "    \n",
    "def impute_na(df, na_feats, method=\"median\"):\n",
    "    \"\"\"Imputes missing data.\"\"\"\n",
    "    df = df.copy()\n",
    "    if method == \"median\":\n",
    "        for f in na_feats:\n",
    "            median_val = df[f].median()\n",
    "            df[f] = df[f].fillna(median_val)\n",
    "    elif method == \"mode\":\n",
    "        for f in na_feats:\n",
    "            mode_val = df[f].mode()[0]\n",
    "            df[f] = df[f].fillna(mode_val)\n",
    "    elif method == \"mean\":\n",
    "        for f in na_feats:\n",
    "            mean_val = df[f].mean()\n",
    "            df[f] = df[f].fillna(mean_val)\n",
    "    return df\n",
    "\n",
    "def model_performance(clf, X_features, y_target, t=\"Training Set\"):\n",
    "    \"\"\"Provides information about the performance of classification model.\"\"\"\n",
    "    labels=[\"No\",\"Yes\"]\n",
    "    print(colored(t,\"blue\"))\n",
    "    predictions = clf.predict(X_features)\n",
    "    predictions_prob = clf.predict_proba(X_features)[:,1]\n",
    "    \n",
    "    auc_score = np.round(roc_auc_score(y_target,predictions_prob)*100, 2)\n",
    "    print(f\"ROC AUC Score: {auc_score}\")\n",
    "    f1_weighted = np.round(f1_score(y_target,predictions,pos_label=\"Yes\",average=\"weighted\")*100,2)\n",
    "    print(f\"Weighted Average Accuracy: {f1_weighted}\")\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    cfmap = confusion_matrix(y_true=y_target,  \n",
    "                             y_pred=predictions)\n",
    "    \n",
    "    sns.heatmap(pd.DataFrame(cfmap), annot = True, xticklabels=labels, yticklabels=labels, \n",
    "                fmt='d', annot_kws={\"size\": 15})\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"getdata\"></a>\n",
    "# 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, HR dataset was sourced from [IBM HR Analytics Employee Attrition & Performance](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) which contains employee data with various information about the employees. I will use this dataset to predict when employees are going to quit by understanding the main drivers of employee churn. <br>\n",
    "\n",
    "> As stated on the [IBM website](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) **\"This is a fictional data set created by IBM data scientists\"**. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data and displaying first 5 rows:\n",
    "csv_path = \"https://ibm.box.com/shared/static/m2sr719gi1ametjwmbqstka14ohxmpin.csv\"\n",
    "data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Data shape:\",\"blue\"),f\"{data.shape}.\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset contains 1470 rows (observations) and 35 columns. Each row is a record for an individual employee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataexplore\"></a>\n",
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **We will analyse the dataset to identify:**\n",
    "- Target variable\n",
    "- Features with missing values\n",
    "- Numerical variables: discrite & continuous and their distribution\n",
    "- Categorical variables\n",
    "- Potential relationship between the variables and the target: Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    ">Attrition indicates if the employee is currently active ('No') or has left the company ('Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of Current Employees is {:.1f}% and of Ex-employees is: {:.1f}%\".format(\n",
    "    sum(data['Attrition'] == 'No')/ len(data)*100,\n",
    "    sum(data['Attrition'] == 'Yes')/ len(data)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Machine Learning algorithms typically work best when the number of instances of each classes are roughly equal. We will have to address this imbalanced classes issue in the Modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = data.columns.tolist()\n",
    "num_feats = [feat for feat in feats if data[feat].dtype != \"O\"]\n",
    "cat_feats = [feat for feat in feats if ((data[feat].dtype == \"O\") and (feat != \"Attrition\"))]\n",
    "na_feats = [feat for feat in feats if data[feat].isnull().sum()>0]\n",
    "discrete_feats = [feat for feat in num_feats if data[feat].nunique()<=10]\n",
    "cont_num_feats = [feat for feat in num_feats if data[feat].nunique()>10]\n",
    "\n",
    "print(colored(\"Number of features:\",\"blue\"), f\"{len(feats)}; { num_feats[:3]}\")\n",
    "print(colored(\"Number of numerical features:\",\"blue\"), f\"{len(num_feats)}; { num_feats[:3]}\")\n",
    "print(colored(\"Number of continuous numeric features:\",\"blue\"), f\"{len(cont_num_feats)}; {cont_num_feats[:3]}\")\n",
    "print(colored(\"Number of discrete numeric features:\",\"blue\"),f\"{len(discrete_feats)}; {discrete_feats[:3]}\")\n",
    "print(colored(\"Number of categorical features:\",\"blue\"), f\"{len(cat_feats)}; {cat_feats[:3]}\")\n",
    "print(colored(\"Number of features with missing values:\",\"blue\"),f\"{len(na_feats)}; {na_feats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset contains several numerical and categorical columns providing various information on employee's personal and employment details.\n",
    "\n",
    "> There are 3 features that containe missing values: \n",
    "- DistanceFromHome\n",
    "- JobSatisfaction\n",
    "- WorkLifeBalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cont_num_feats].hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the histograms for continuous numerical features:\n",
    " - Many histograms are tail-heavy; indeed several distributions are right-skewed (e.g. MonthlyIncome DistanceFromHome, YearsAtCompany). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n",
    " - Age distribution is a slightly right-skewed normal distribution with a lot of staff being between 25 and 45 years old.\n",
    " - EmployeeNumber is likely to be a unique identifier for employees given the feature's quasi-uniform distribution. It is a redundant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_feats=list()\n",
    "redundant_feats.extend([\"EmployeeNumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Analyzing relathionshop between MonthlyIncome and Attrition \n",
    "g = sns.FacetGrid(data, col='Attrition',size=4,aspect=1.2)\n",
    "g.map(plt.hist,\"MonthlyIncome\", density=True,bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Employees with higher income are less likely to leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[discrete_feats].hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the histograms for discrete numerical features:\n",
    " - EmployeeCount and StandardHours are constant values for all employees. They're redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding \"EmployeeCount\",\"StandardHours\" to redundant features\n",
    "redundant_feats.extend([\"EmployeeCount\",\"StandardHours\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing relationship between discrete numeric features and Attrition \n",
    "analyse_discrete(data, feats=[f for f in discrete_feats if f not in redundant_feats],target=\"Attrition\", \n",
    "                 plot_title=\"Percentage of Leavers by Category\",grid_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some observations about the relationship between discrete numeric features and Attrition: \n",
    "- Employees with lower JobLevel, JobSatisfaction, and JobInvolvement are more likely to leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_feats].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the information for categorical features:\n",
    " - Over18 has constant values for all employees. This is a redundant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding \"Over18\" to redundant features\n",
    "redundant_feats.extend([\"Over18\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing relathionshop between categorical features and Attrition \n",
    "analyse_discrete(data, feats=[f for f in cat_feats if f not in redundant_feats],target=\"Attrition\", \n",
    "                 plot_title=\"Percentage of Leavers by Category\",grid_width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some observations about the relationship between categorical features and Attrition: \n",
    "- Gender distribution shows that the dataset features a higher relative proportion of male ex-employees than female ex-employees, with normalised gender distribution of ex-employees in the dataset at 17.0% for Males and 14.8% for Females.\n",
    "- Employees who are single, work overtime, and travel frequently are more likely to leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Featureeng\"></a>\n",
    "# 4. Feature Engineering\n",
    "\n",
    ">In this section, we undertake data pre-processing steps to prepare the datasets for Machine Learning algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ibm.box.com/shared/static/g9awggbgx6lbn1x58ahewq1hcua1d0l7.png\" alt=\"ohenc\" style=\"width:900px;height:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine percentage of missing values\n",
    "for f in na_feats:\n",
    "    print(f\"\\n{f}\\n Number of Missing Values: {np.sum(data[f].isnull())}.\\\n",
    "    Percentage of Missing Values: {np.round(np.sum(data[f].isnull()*100/len(data)))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = impute_na(data,na_feats=na_feats,method=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there are missing values left\n",
    "for f in na_feats:\n",
    "    print(f\"\\n{f}\\n Number of Missing Values: {np.sum(data[f].isnull())}.\\\n",
    "    Percentage of Missing Values: {np.round(np.sum(data[f].isnull()*100/len(data)))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding - One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_feats].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ibm.box.com/shared/static/d5ltrcmsvuv484q2ngcwvbmr1kmawvmv.png\" alt=\"ohenc\" style=\"width:900px;height:300px\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding categorical features\n",
    "data_enc = pd.get_dummies(data[cat_feats], drop_first=True)\n",
    "## Merging with initial data\n",
    "data = data.join(data_enc)\n",
    "## Dropping initial categorical features\n",
    "data.drop(columns=cat_feats,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the processed data is: {data.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feature Scaling using MinMaxScaler essentially shrinks the range such that the range is now between 0 and n. Linear Machine Learning algorithms perform better when input numerical variables fall within a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "scale_col = data.columns.to_list()\n",
    "scale_col.remove('Attrition')\n",
    "data_scaled = data.copy()\n",
    "\n",
    "for col in  scale_col:\n",
    "    data_scaled[col] = scaler.fit_transform(data_scaled[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Featureselec\"></a>\n",
    "# 5. Feature Selection\n",
    "> There are multiple methods for feature selection:\n",
    "- Filter\n",
    "- Wrapper\n",
    "- Embedded<br>\n",
    "For this use case we will use Filter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reduntant features: {redundant_feats}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = [f for f in data_scaled.columns.to_list() if f != \"Attrition\"]\n",
    "selected_feats = [f for f in all_feats if f not in redundant_feats]\n",
    "target = \"Attrition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of selected features: {len(selected_feats)}. \\nTarget variable is: {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling\"></a>\n",
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for Training (80%) and Testing (20%)\n",
    "- Training data used to define the model\n",
    "- Testing data is kept to one side when the model is defined and applied to the model to validate it\n",
    "- Testing data is a proxy for new data in models that need to work with as-yet unseen input\n",
    "\n",
    "> Since we have imbalanced classes problem: i.e. more current employees than employees that left, let's use stratify=y to maintain the same ratio when splitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled[selected_feats], data_scaled[target],\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=7, \n",
    "                                                    stratify=data_scaled[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}. y_train shape: {y_train.shape}.\")\n",
    "print(f\"X_test shape: {X_test.shape}. y_test shape: {y_test.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting various algorithms to try\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(random_state=7,class_weight='balanced')))\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=100, random_state=7)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))\n",
    "models.append(('Decision Tree Classifier',DecisionTreeClassifier(random_state=7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "![AI](https://www.researchgate.net/profile/Johar_Ashfaque/publication/332370436/figure/fig1/AS:746775958806528@1555056671117/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# Table to populate with performance results\n",
    "df_results = pd.DataFrame(columns=['Algorithm', 'Mean ROC AUC', 'Weighted Mean Accuracy'])\n",
    "\n",
    "# Evaluate each model\n",
    "for i, (name, model) in enumerate(models):\n",
    "    kfold = KFold(n_splits=10, random_state=7)  # 10-fold cross-validation\n",
    "\n",
    "    cv_acc_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='f1_weighted')\n",
    "    cv_auc_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    df_results.loc[i] = [name, round(cv_auc_results.mean()*100, 2), round(cv_acc_results.mean()*100, 2)]\n",
    "\n",
    "    df_results.sort_values(by=['Weighted Mean Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by=['Weighted Mean Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Random Forest Classifier\n",
    "**Random Forest** is a popular and versatile machine learning method that is capable of solving both regression and classification problems. It is an ensemble algorithm, as it relies on an ensemble of decision trees. A decision tree is composed of a series of decisions that can be used to classify an observation in a dataset.\n",
    "\n",
    "Random Forest fits a number of decision tree classifiers on various **sub-samples of the dataset** and use **averaging** to improve the predictive accuracy and control over-fitting. Random Forest can handle a large number of features, and is helpful for estimating which of your variables are important in the underlying data being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ibm.box.com/shared/static/b5mrbybamus8j94q3w6y6vzrwoqvzkzx.png\" alt=\"conf\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(class_weight = \"balanced\", random_state=7)\n",
    "rf_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeleval\"></a>\n",
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion matrix</h2></center>\n",
    "\n",
    "<center><img src=\"https://skappal7.files.wordpress.com/2018/08/confusion-matrix.jpg?w=748\" alt=\"conf\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance(rf_classifier,X_test,y_test,t=\"Testing Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model does much better on the training set than on the testing set, hence doesn’t generalize well from our training data to unseen data.\n",
    " This problem is called **Overfitting**. We will do HyperParameter Optimization to overcome this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight = \"balanced\", random_state=7)\n",
    "param_grid = {'n_estimators': [50, 75, 100, 125],\n",
    "              'min_samples_split':[2,4,6,8,10],\n",
    "              'min_samples_leaf': [1, 2, 3, 4],\n",
    "              'max_depth': [5, 10, 15]}\n",
    "\n",
    "grid_obj = GridSearchCV(rf,\n",
    "                        iid=True,\n",
    "                        return_train_score=True,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='roc_auc',\n",
    "                        cv=10,n_jobs=-1)\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "rf_opt = grid_fit.best_estimator_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(grid_obj.best_estimator_))\n",
    "print(\"best params: \" + str(grid_obj.best_params_))\n",
    "print('best score:', grid_obj.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance(rf_opt,X_test,y_test,t=\"Testing Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have successfully increased our AUC score from 76% to 81% closing the gap between model's performance on training and testing sets. Moreover, Weighted Average Accuracy has increased from 78% to 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Inference and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "feat_importances = pd.Series(rf_opt.feature_importances_, index=selected_feats)\n",
    "feat_importances.nlargest(9).plot(kind='barh').invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Indicators of Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Monthly Income** - People on higher wages are less likely to leave the company. Hence, efforts should be made to gather information on industry benchmarks in the current local market to determine if the company is providing competitive wages.\n",
    "\n",
    "\n",
    "- **Over Time** - People who work overtime are more likelty to leave the company. Hence, efforts must be taken to appropriately scope projects upfront with adequate support and manpower so as to reduce the use of overtime.\n",
    "\n",
    "\n",
    "- **Age** - Employees in relatively young age bracket 25-35 are more likely to leave. Hence, efforts should be made to clearly articulate the long-term vision of the company and young employees fit in that vision, as well as provide incentives in the form of clear paths to promotion for instance.\n",
    "\n",
    "\n",
    "- **DistanceFromHome** - Employees who live further from home are more likely to leave the company. Hence, efforts should be made to provide support in the form of company transportation for clusters of employees leaving the same area, or in the form of Transportation Allowance.\n",
    "\n",
    "\n",
    "- **TotalWorkingYears** - The more experienced employees are less likely to leave. Employees who have between 5-8 years of experience should be identified as potentially having a lower-risk of leaving.\n",
    "\n",
    "\n",
    "- **YearsAtCompany** - Loyal employees are less likely to leave. Employees who hit their two-year anniversary should be identified as potentially having a lower-risk of leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the company generates more data on its employees (on new hires and recent leavers), the algorithm can be re-trained using the additional data and theoritically generate more accurate predictions.\n",
    "Employees can be assigned to a \"Risk Category\" based on the predicted label such that:\n",
    "- **Low-risk** - employees with Attrition probability < 0.6\n",
    "- **Medium-risk** - employees with Attrition probability l between 0.6 and 0.8\n",
    "- **High-risk** - employees with Attrition probability > 0.8 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategic Retention Plan\n",
    "\n",
    "A strategic **\"Retention Plan\"** should be drawn for each **Risk Category** group. For example:\n",
    "\n",
    "- Face-to-face meetings between a HR representative and employees can be initiated for **medium-** and **high-risk employees** to discuss work conditions. \n",
    "- Also, a meeting with those employee's Line Manager would allow to discuss the work environment within the team and whether steps can be taken to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to Top</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
